{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "89666ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0bc3458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 35.93it/s]\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(\"..\", \"docs\") \n",
    "loader = DirectoryLoader(\n",
    "    path,\n",
    "    glob=\"**/*.md\",\n",
    "    loader_cls=UnstructuredMarkdownLoader,\n",
    "    show_progress=True \n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "91ff5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# I chose not to use text splitter at this point because document size is small.\n",
    "# Generating Embeddings and storing in vector Database FAISS\n",
    "vector_store = FAISS.from_documents(documents=documents,embedding=embedding_model)\n",
    "vector_store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67333acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"conversational\"\n",
    ") # type: ignore\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00725d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are an expert Site Reliability Engineer. \n",
    "        Your task is to create a clear, step-by-step troubleshooting runbook based on the provided context. \n",
    "        The output must be actionable, not just summary text. For example: \n",
    "Input: \n",
    "“I’m seeing increased memory usage on pod X” \n",
    "Output: \n",
    "○ Step 1: Check container memory limits and requests using kubectl describe \n",
    "pod \n",
    "○ Step 2: Compare historical memory usage from Prometheus or Datadog \n",
    "○ Step 3: Look for memory leaks in the application logs \n",
    "○ Step 4: Consider restarting the pod if memory is consistently breaching limits\"\"\"),\n",
    "        \n",
    "        (\"human\", \"\"\"Here is the context from our internal documents:\n",
    "        ---\n",
    "        {context}\n",
    "        ---\n",
    "        Now, please generate a runbook for the following problem: {question}\"\"\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a84feac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Runbook for \"My pod keeps restarting and the events show 'liveness probe failed'.\"**\n",
       "\n",
       "**Step 1: Check Liveness Probe Configuration**\n",
       "\n",
       "1. Run `kubectl describe pod <pod-name>` to view the events and configuration of the pod.\n",
       "2. Look for the liveness probe configuration under the \"Readiness Gates\" or \"Liveness\" section.\n",
       "3. Check the probe's timeout, period, and command to determine if it's too aggressive.\n",
       "\n",
       "**Step 2: Verify Downstream Dependencies**\n",
       "\n",
       "1. Inspect the application code to determine if it's making any downstream calls.\n",
       "2. Check if the downstream call is slow and if it's causing the liveness probe to fail.\n",
       "3. Consider implementing a mechanism to cache or throttle these calls to improve performance.\n",
       "\n",
       "**Step 3: Analyze Application Logs**\n",
       "\n",
       "1. Run `kubectl logs <pod-name>` to view the application logs.\n",
       "2. Search for errors or exceptions related to the liveness probe failure.\n",
       "3. Check for any signs of memory leaks or resource exhaustion.\n",
       "\n",
       "**Step 4: Consider Increasing Liveness Probe Timeout**\n",
       "\n",
       "1. If the liveness probe is too aggressive, consider increasing the timeout or period.\n",
       "2. Run `kubectl patch deployment <deployment-name> --patch '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"<container-name>\",\"livenessProbe\":{\"timeoutSeconds\":5,\"periodSeconds\":10}}]}}}}'` to update the liveness probe configuration.\n",
       "3. Monitor the pod's behavior and adjust the configuration as needed.\n",
       "\n",
       "**Step 5: Force a Rolling Restart (Optional)**\n",
       "\n",
       "1. If the problem persists after increasing the liveness probe timeout, consider forcing a rolling restart of the deployment.\n",
       "2. Run `kubectl rollout restart deployment/<deployment-name>` to restart all pods in the deployment.\n",
       "3. Monitor the pod's behavior and adjust the configuration as needed.\n",
       "\n",
       "**Step 6: Verify HPA Configuration (Optional)**\n",
       "\n",
       "1. If the HPA is not scaling the deployment correctly, check the HPA configuration.\n",
       "2. Run `kubectl describe hpa <hpa-name>` to view the HPA configuration.\n",
       "3. Verify that the HPA is configured correctly and that the metrics-server is running correctly.\n",
       "\n",
       "**Step 7: Re-Install Metrics-Server (Optional)**\n",
       "\n",
       "1. If the metrics-server is not running correctly, re-install it.\n",
       "2. Run `kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.1/components.yaml` to re-install the metrics-server.\n",
       "3. Verify that the HPA is now scaling the deployment correctly."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided context and the problem description, here's a step-by-step troubleshooting runbook:\n",
       "\n",
       "**Problem:** Pod memory usage is consistently high, potentially causing memory pressure on the node and pod eviction.\n",
       "\n",
       "**Runbook:**\n",
       "\n",
       "○ **Step 1: Check container memory limits and requests using kubectl describe pod**\n",
       "\n",
       " Run the following command to check the pod's configuration:\n",
       "```bash\n",
       "kubectl describe pod <pod_name>\n",
       "```\n",
       "Look for the \"Resources\" section and verify that the container has memory limits and requests set. Make a note of the values.\n",
       "\n",
       "○ **Step 2: Compare historical memory usage from Prometheus or Datadog**\n",
       "\n",
       "Access the Prometheus or Datadog UI to view historical memory usage data for the pod. Compare the current memory usage to previous values to identify any patterns or anomalies.\n",
       "\n",
       "○ **Step 3: Look for memory leaks in the application logs**\n",
       "\n",
       " Review the application logs to check for any signs of memory leaks. Look for error messages or patterns that suggest excessive memory usage.\n",
       "\n",
       "○ **Step 4: Check if the pod is configured to use excessive resources**\n",
       "\n",
       " Verify that the pod's configuration is not intentionally set to use excessive resources. Check the pod's YAML file and the deployment or replica set configuration to ensure that the memory limits and requests are set correctly.\n",
       "\n",
       "○ **Step 5: Consider restarting the pod if memory is consistently breaching limits**\n",
       "\n",
       " If the memory usage is consistently high and causing issues, consider restarting the pod to reset its memory usage. Use the following command to restart the pod:\n",
       "```bash\n",
       "kubectl rollout restart deployment <deployment_name>\n",
       "```\n",
       "Replace `<deployment_name>` with the actual name of the deployment associated with the pod.\n",
       "\n",
       "○ **Step 6: Review and adjust resource requests and limits**\n",
       "\n",
       " Based on the previous steps, review and adjust the resource requests and limits for the pod as necessary. Make sure to set the requests equal to the limits for critical workloads to get a Guaranteed QoS class.\n",
       "\n",
       "○ **Step 7: Run a memory leak detection tool (optional)**\n",
       "\n",
       " If the issue persists, consider running a memory leak detection tool, such as Valgrind or AddressSanitizer, to identify any memory leaks in the application code.\n",
       "\n",
       "By following these steps, you should be able to troubleshoot and resolve the issue with high pod memory usage."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = None\n",
    "\n",
    "db = FAISS.load_local(\n",
    "    os.path.join(\"..\",\"faiss_index\"),\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True \n",
    ")\n",
    "\n",
    "\n",
    "while True:\n",
    "    query = input(\"Enter your query here : \")\n",
    "    if query == \"exit\":\n",
    "        break\n",
    "\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    similar_docs = db.similarity_search_by_vector(query_embedding, k=3)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in similar_docs])\n",
    "\n",
    "    prompt = prompt_template.invoke({\"question\":HumanMessage(query),\"context\":context})\n",
    "    display(Markdown(chat_model.invoke(prompt).content))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111377b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
